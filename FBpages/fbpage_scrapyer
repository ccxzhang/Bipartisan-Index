import os
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import urllib.request
import time

## Get current directory path
path= os.getcwd()

## Construct the driver
service = Service("/Applications/chromedriver")
service.start()
driver = webdriver.Remote(service.service_url)
driver.get('http://www.facebook.com')

## Get and insert the email and passport
email = driver.find_element_by_id("email")
password = driver.find_element_by_id("pass")
email.send_keys('YourFBAccount')
password.send_keys('Yourpassowrd')
password.submit()

## Have a glimpse of the page
time.sleep(5)

## Get the fans page
driver.get("https://www.facebook.com/Louisa.Mak.Ming.Sze")

## Use selenuim to scroll down
for x in range(1, 10):
    driver.execute_script("window.scrollTo(0,document.body.scrollHeight)")
    time.sleep(5)

## Use bs to get the page content
soup= BeautifulSoup(driver.page_source, 'html.parser')

## Write a function to acquire all images
def getImage():

    os.mkdir('images')

    img_list= list()
    images= soup.findAll("img")
    ## Store the image url into the list
    for image in images:
        if "scontent" in image['src']:
            img_list.append(image['src'])
        else:
            pass

    for i,j in enumerate(img_list):
        filename= path+ "/images/louisamak_" + str(i) + ".png"
        urllib.request.urlretrieve(j, filename)

getImage()


## Get the posts
titles= soup.find_all("div", {"class": "kvgmc6g5 cxmmr5t8 oygrvhab hcukyx3x c1et5uql ii04i59q"})

for title in titles:
    post= title.find('div', {'dir': 'auto'})
    print(post.getText())

driver.quit()
